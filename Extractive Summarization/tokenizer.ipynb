{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa07ea9",
   "metadata": {},
   "source": [
    "## Training a Hindi Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767cc078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /usr/local/anaconda3/lib/python3.8/site-packages (0.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f76ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168f6acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hindi_data/hin_wikipedia_2021_30K-co_n.txt', 'hindi_data/hin_wikipedia_2021_30K-sentences.txt', 'hindi_data/hin_wikipedia_2021_30K-inv_so.txt', 'hindi_data/hin_wikipedia_2021_30K-sources.txt', 'hindi_data/hin_wikipedia_2021_30K-words.txt', 'hindi_data/hin_wikipedia_2021_30K-inv_w.txt', 'hindi_data/hin_wikipedia_2021_30K-co_s.txt']\n"
     ]
    }
   ],
   "source": [
    "paths = [str(x) for x in Path(\"./hindi_data/\").glob(\"**/*.txt\")]\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b35444",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ab6e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files='hindi_data/hin_wikipedia_2021_30K-words.txt', vocab_size=52000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f60b8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ByteLevelBPETokenizer' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e08d9e103d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hindiBERTo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ByteLevelBPETokenizer' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\".\", \"hindiBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71873cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'à¤ĩ', 'à¤Ĥ', 'à¤¦', 'à¥į', 'à¤°à¤§à¤¨', 'à¥ģ', 'à¤·', 'Ġà¤®', 'à¥ĩà¤Ĥ', 'Ġ7', 'Ġà¤°', 'à¤Ĥ', 'à¤Ĺ', 'Ġà¤¹', 'à¥ĭ', 'à¤¤', 'à¥ĩ', 'Ġà¤¹', 'à¥Īà¤Ĥ', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    './hindiBERTo/hindiBERTo-vocab.json',\n",
    "    './hindiBERTo/hindiBERTo-merges.txt',\n",
    ")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "tokens = (tokenizer.encode(\"इंद्रधनुष में 7 रंग होते हैं\"))\n",
    "\n",
    "print(tokens.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e13487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HindiDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            './hindiBERTo-vocab.json',\n",
    "            './hindiBERTo-merges.txt',\n",
    "        )\n",
    "\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"🔥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a788405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/05/2022 15:08:09 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "02/05/2022 15:08:09 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:10 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json not found in cache or force_download set to True, downloading to /Users/anjaneyatripathi/.cache/torch/transformers/tmpbjzbja_5\n",
      "02/05/2022 15:08:10 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "Downloading: 100%|█████████████████████████████| 466/466 [00:00<00:00, 75.9kB/s]\n",
      "02/05/2022 15:08:12 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json in cache at /Users/anjaneyatripathi/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9\n",
      "02/05/2022 15:08:12 - INFO - transformers.file_utils -   creating metadata file for /Users/anjaneyatripathi/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9\n",
      "02/05/2022 15:08:12 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json from cache at /Users/anjaneyatripathi/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9\n",
      "02/05/2022 15:08:12 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "02/05/2022 15:08:12 - INFO - transformers.tokenization_utils_base -   Model name 'distilbert-base-multilingual-cased' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'distilbert-base-multilingual-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "02/05/2022 15:08:12 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:13 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:14 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:14 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:15 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:17 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "02/05/2022 15:08:17 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-tokenizer.json not found in cache or force_download set to True, downloading to /Users/anjaneyatripathi/.cache/torch/transformers/tmpeqqjobni\n",
      "02/05/2022 15:08:17 - INFO - requests.packages.urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "Downloading: 100%|█████████████████████████| 1.96M/1.96M [00:01<00:00, 1.19MB/s]\n",
      "02/05/2022 15:08:20 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-tokenizer.json in cache at /Users/anjaneyatripathi/.cache/torch/transformers/50b8b7e8bf1d4b8f0ff3483954e688e08a96c58bfafbe59681216547db309360.9d1d8734866bca37e97ed71433cc55f0caf5beda4316d8a986007286555fc1c4\n",
      "02/05/2022 15:08:20 - INFO - transformers.file_utils -   creating metadata file for /Users/anjaneyatripathi/.cache/torch/transformers/50b8b7e8bf1d4b8f0ff3483954e688e08a96c58bfafbe59681216547db309360.9d1d8734866bca37e97ed71433cc55f0caf5beda4316d8a986007286555fc1c4\n",
      "02/05/2022 15:08:20 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-vocab.json from cache at None\n",
      "02/05/2022 15:08:20 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-merges.txt from cache at None\n",
      "02/05/2022 15:08:20 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-added_tokens.json from cache at None\n",
      "02/05/2022 15:08:20 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-special_tokens_map.json from cache at None\n",
      "02/05/2022 15:08:20 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-tokenizer_config.json from cache at None\n",
      "02/05/2022 15:08:20 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-tokenizer.json from cache at /Users/anjaneyatripathi/.cache/torch/transformers/50b8b7e8bf1d4b8f0ff3483954e688e08a96c58bfafbe59681216547db309360.9d1d8734866bca37e97ed71433cc55f0caf5beda4316d8a986007286555fc1c4\n",
      "Traceback (most recent call last):\n",
      "  File \"run_lm_training.py\", line 717, in <module>\n",
      "    main()\n",
      "  File \"run_lm_training.py\", line 634, in main\n",
      "    tokenizer = tokenizer_class.from_pretrained(\n",
      "  File \"/usr/local/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1140, in from_pretrained\n",
      "    return cls._from_pretrained(*inputs, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 1287, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.8/site-packages/transformers/tokenization_roberta.py\", line 150, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/anaconda3/lib/python3.8/site-packages/transformers/tokenization_gpt2.py\", line 157, in __init__\n",
      "    with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
      "TypeError: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    }
   ],
   "source": [
    "!python run_lm_training.py \\\n",
    "    --output_dir=./models/hindiBERTo-v1 \\\n",
    "    --model_type=roberta \\\n",
    "    --model_name_or_path=distilbert-base-multilingual-cased \\\n",
    "    --do_train \\\n",
    "    --train_data_file=./hindi_data/hin_wikipedia_2021_30K-sentences.txt \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=./hindi_data/hin_wikipedia_2021_30K-sentences.txt \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f300c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
